# 詳細設計.md

バージョン: v0.5  (更新日: 2025-08-20)  ※v0.4→v0.5 主変更: プロセス join timeout 時 terminate/kill フォールバック実装 / ping RTT (last_rtt_ms) 記録追加 / OrchestratorConfig フラグ拡張 (stop_grace_wait_sec, simulate_hang_on_stop, enable_central_logging placeholder) / 試験: terminate 経路・RTT テスト追加。
参照: `要件.md`, `基本設計.md`, `CODING_STYLE.md`

> 本書はクラス/関数/データ構造/制御フロー/例外仕様/テスト観点を精緻化し、GitHub Copilot に直接投入できる粒度へ分解する。実装フェーズでは本記述との差異は必ず更新。

---
## 1. 実装ディレクトリ構成 (案)
```
app/
  main.py                      # エントリーポイント (CLI → Orchestrator 起動)
  scripts/
    config/                    # 設定読み込みと検証
      loader.py                # ConfigLoader 実装
      schema.py                # バリデーション定義 (必要時)
    core/                      # コアランタイム (プロセス/IPC/集約)
      orchestrator.py          # Orchestrator (ResultDispatcherThread 内部保有)
      worker.py                # CaptureInferenceWorker
      aggregator.py            # Aggregator / RingBuffer
      recorder.py              # Recorder
      models.py                # データモデル定義 (抽象仕様)
      errors.py                # 例外種別
      utils_time.py            # 時刻/計測ユーティリティ
      messages.py              # IPC メッセージ型 (抽象インタフェース記述のみ)
      logging_setup.py         # 集中ロギング初期化
      metrics.py               # MetricsThread (統計/ストール検知)
    gui/
      adapter.py               # GUIAdapter (内部状態→表示用 DTO)
      app_gui.py (後続)
  tools/
    convert_model.py           # モデル変換ツール
  logs/ (生成)                 # 実行時ログ
  results/                     # エクスポート/録画
  models/                      # IRファイル配置
  resources/
    ApplicationConfig.xml
```

---
## 2. データモデル / メッセージ抽象仕様
実装コードは本書に記載しない。以下はフィールド定義レベルの抽象仕様。

### 2.1 推論結果 (`ResultRecord`)
- camera_id: str
- timestamp_utc: datetime (UTC, ISO8601 出力想定)
- gesture_label: str
- confidence: float (0.0–1.0)
- latency_ms: float? (前処理+推論区間。計測不能時 None)

### 2.2 設定サブ構造例
- CameraConfig: id, url
- ModelConfig: ir_xml, ir_bin, metadata
- InferenceConfig: target_fps, device
- RetryConfig: connect_max_attempts, connect_backoff_sec (指数/線形選択は loader で確定)
- BufferConfig: results_max_entries
- RecordingConfig: enabled, output_dir
- ExportConfig: default_format
- GUIConfig: theme
- LoggingConfig: dir, level, rotate_max_mb, rotate_backups
Config ルートは上記を集約し基本イミュータブル(再代入禁止)。

### 2.3 IPC メッセージ種別 (messages.py)
| 名称 | 方向 | 主要フィールド | 説明 |
|------|------|---------------|------|
| ControlMessage | 親→子 | type(START/STOP/RELOAD/PING), payload(dict) | 制御指示 |
| StatusUpdate | 子→親 | camera_id, status(INIT/RUNNING/RETRYING/DOWN/EXITING), attempts, last_error? | 状態通知 |
| StatsMessage | 子→親 | camera_id, fps, avg_latency_ms?, drop_rate? | 周期統計 |
| ExitNotice | 子→親 | camera_id, code(0=正常), reason | 終了通知 (ControlMessage に EXITING は存在しない) |

**役割境界明確化:**
- **StatsMessage**: 各Workerプロセス内で1秒間隔で集計されるローカル統計（FPS、平均レイテンシ、ドロップ率）
- **MetricsThread**: メインプロセスでStatsMessageを受信し、中央での異常判定（カメラストール検知、閾値超過警告）とログ出力を担当

シリアライズ: Python 標準 pickle (初期) → パフォーマンス要件で msgpack 検討 (オープン事項 D-05)。

### 2.4 Ping/Pong ヘルス監視 (v0.5 更新)
実装ステージ: プロセス化前の **スレッドMVP**。Pipe ではなく `queue.Queue` による単方向 (親→子) 制御で PING を送出し、子は `StatusUpdate.ping_response` により PONG を返す。

フロー:
1. Orchestrator 内 `PingThread` が `ping_interval_sec` ごとに各カメラ control_queue へ `ControlMessage(type=PING, payload={"id": <timestamp_ms>})` を投入。
2. Worker はフレーム生成ループ各周回で `_process_control()` を非ブロッキング実行し、受信した PING に対し即座に `StatusUpdate(status="RUNNING", ping_response=id)` を result_queue へ送信。
3. Orchestrator の `ResultDispatcher` は StatusUpdate を受信し、当該 camera の `_ping_state[last_id]==ping_response` であれば `responded=True` および `losses=0` にリセット。
4. 次の interval 時点で `responded=True` なら新規 PING を送信。`responded=False` で `sent_ts` から `ping_timeout_sec` 超過した場合は `losses++` し `PING_TIMEOUT` ログを出力、閾値 `ping_loss_threshold` 到達で `CAMERA_DOWN` ログ。

v0.5 追加:
- ラウンドトリップ遅延 (RTT) を `last_rtt_ms` として測定 (PING 送信時刻→対応する pong 受信時刻差 *1000)。
- DOWN からの復帰時 CAMERA_RECOVER ログを出力。

設計差異/未実装事項:
- DOWN 状態は引き続き StatusUpdate へ反映せず (将来: StatusUpdate.status="DOWN")。
- control_queue が満杯時の再試行は行わず WARN ログ一回 (低頻度想定)。

テスト指針:
- 正常応答: losses が常に 0。
- 応答停止 (respond_to_ping=False) で losses が閾値以上に増加し CAMERA_DOWN ログが出る (現行は losses 値を検証)。
- 短 interval/timeout 設定でユニットテスト高速化 (<1s)。

---
## 3. 主要クラス詳細仕様
### 3.1 ConfigLoader (loader.py)
| 項目 | 内容 |
|------|------|
| 目的 | ApplicationConfig.xml を読み込み `Config` オブジェクトを返却 |
| 入力 | Path (XMLファイル) |
| 出力 | Config (成功) |
| 例外 | ValidationError (不足/不正), FileNotFoundError |
| ログ | INFO: 読込開始/完了, ERROR: 失敗詳細 |
| 処理 | XML→dict→型変換→値検証(必須/範囲)→Config生成 |

擬似コード:
```python
def load(path: Path) -> Config:
    xml = _read_xml(path)
    raw = _xml_to_dict(xml)
    _validate(raw)
    return _to_config(raw)
```

### 3.2 Orchestrator (orchestrator.py)
| 項目 | 内容 |
|------|------|
| 目的 | 子プロセス生成/監視/停止制御 |
| 状態 | config, workers(dict[camera_id, ProcessHandle]), aggregator, stop_event, result_queue, log_queue, control_conns, metrics_thread, result_dispatcher_thread |
| 主要メソッド | start(), stop(), spawn_worker(camera_id), broadcast(msg)->bool, _monitor_loop(), _run_dispatcher() |
| 例外 | モデル/設定致命エラー伝播で SystemExit |
| ログ | INFO: 起動/停止, WARNING: 再起動試行, ERROR: 子異常終了 |
| 制御 | start(): ログ初期化→result_queue 消費スレッド開始→各カメラ spawn→監視スレッド起動→(GUI後) |
| 終了 | stop(): broadcast(STOP)→短い猶予sleep(ExitNotice flush)→worker JOIN→stop_event.set()→dispatcher/metrics/ping thread JOIN (SENTINEL 不要) |
| 再起動管理 | restart_history(各camera, 300s窓) をリング保持し閾値超過で status=DOWN 固定 |
| ヘルス管理 | ping 送信 / 連続 timeout カウンタ保持 (threshold=3) で DOWN 判定 |

#### v0.5 追加仕様補足
| 項目 | 追加内容 |
|------|----------|
| terminate fallback | process join timeout 後 terminate→kill シーケンス |
| last_rtt_ms | ping RTT を ms で保持 (float) |
| active_process_count | 生存プロセス数 (テスト/監視用) |
| stop_grace_wait_sec | STOP ブロードキャスト後 flush 猶予秒 |
| simulate_hang_on_stop | テスト用ハング誘発フラグ |
| enable_central_logging | 集中ロギング (後続実装) プレースホルダ |

集中ロギング (今後 v0.6 予定):
1. Orchestrator.start 時に logging_setup.create_listener() で QueueListener 起動。
2. Worker 生成時に QueueHandler を設定する ControlMessage(SETUP_LOGGING) または プロセス entry で直接呼び出し。
3. stop() で listener に sentinel を投入し join。
4. フォーマット: JSON (ts, level, event, camera, msg, pid, proc_name, rtt_ms?)。

再起動戦略 (将来 v0.6):
| 状態 | アクション | 備考 |
|------|-----------|------|
| ping losses≥threshold | optional: 自動 restart | 現状はログのみ |
| プロセス異常終了 (exit code!=0) | 上限回数内で再 spawn | restart_window 秒内回数制限 |
| Kill 経路発生 | 再起動抑制 (手動介入) | 安全性優先 |


シーケンス(起動):
```
main -> ConfigLoader -> Orchestrator.start
  -> for each camera spawn_worker
  -> start GUI loop (非同期)
```

### 3.3 CaptureInferenceWorker (worker.py)
| 項目 | 内容 |
|------|------|
| 目的 | RTSP キャプチャ + 前処理 + OpenVINO 推論 + 結果送信 |
| ループ | while running: capture -> preprocess -> infer -> put_result(result_queue, record) -> 周期的に StatsMessage 送信 |
| 例外 | 初期接続失敗: 再試行; モデルロード失敗: 検知→親へ致命通知 |
| 最適化 | 最新優先: 標準=容量(例:1024)+古い1件破棄→再put (maxsize=1 案は将来最適化検討) |
| メトリクス | 推論時間計測→latency_ms 設定 |
| ログ | QueueHandler 経由 event_code 付与 |

ワーカ主要フロー (抽象):
1. ログキュー初期化 / 初期状態通知 (StatusUpdate INIT)
2. モデルロード (失敗時 致命ログ → 非0終了)
3. ループ:
  - 制御Pipe確認 (STOP 指示で break)
  - フレーム取得 (失敗: 再試行ポリシ適用; 上限超過で StatusUpdate DOWN)
  - 前処理 → 推論 → latency 測定
  - 結果キュー投入 (統一アルゴ: put_nowait→Full なら最古1件 get_nowait 破棄→1回のみ再 put。再失敗は WARNING)
  - 1秒周期で StatsMessage 送信 (fps / 平均レイテンシ / drop_rate)
4. 終了通知 (ExitNotice)

前処理簡略案: リサイズ→色空間変換→正規化。

### 3.4 Aggregator (aggregator.py)
| 項目 | 内容 |
|------|------|
| 目的 | 結果集約・リングバッファ保持・統計計算・エクスポート |
| データ | buffers: dict[camera_id, deque(ResultRecord, maxlen=capacity)], stats_overrides: StatsMessage, ema_fps: dict[camera_id,float] |
| API | push_result(record), apply_stats_message(msg), query(camera_id, since), export(fmt, range, dest), snapshot_stats(now?) |
| 統計 | fps(1s窓), ema_fps(alpha=0.2), avg_latency_ms, latency_p50_ms, latency_p95_ms, drop_rate(override), last_update |
| 計算 | p50/p95 は線形補間 (k=(n-1)*p) |
| 例外 | export 時 I/OError 伝播 (未実装 placeholder) |
| スレッド安全 | 現状単一 dispatcher スレッド想定でロック不要 / 将来マルチプロセス時は親のみアクセス前提 |

### 3.5 Recorder (recorder.py)
| 目的 | 指定カメラ映像とメタ(開始/終了時刻, モデルver) を MP4+JSON 保存 |
| 戦略 | Frame tap: Worker から複製 or Orchestrator 経由ストリーム |
| API | start(camera_id), stop(camera_id) |
| 失敗 | 書込失敗->ERRORログ, 継続不可なら停止通知 |

### 3.6 GUIAdapter (adapter.py)
### 3.7 Logging Setup (logging_setup.py)
| 項目 | 内容 |
|------|------|
| 目的 | 集中ログ (JSON Lines) を構築しマルチプロセス衝突を防止 |
| 初期化 | 親: create log_queue, start QueueListener(thread, handlers=[RotatingFile, Console]) |
| 子 | setup_queue_logging(log_queue) -> root handlers クリア + QueueHandler のみ追加 |
| フォーマット | JSONFormatter (ts ISO8601 UTC, level, event, camera, msg, pid, process_name, extra metrics) |
| 終了 | sentinel(None) を log_queue.put で listener ループ終了 |

### 3.8 Metrics Thread (metrics.py)
周期 (既定 1s) で Aggregator.snapshot_stats() を取得し以下を実施:
1. カメラ毎の最終更新差分を計算し閾値超過で CAMERA_STALL ログ (WARNING)
2. fps/latency/drop_rate を DEBUG (PERF_LATENCY 等) で出力 (必要に応じサンプリング)
3. 高頻度ログ抑制のため遅延/ドロップ閾値 (可設定) を越えた場合のみ追加イベントコード生成

### 3.9 Shutdown Sequence 詳細 (改訂 v0.5)
目的: ExitNotice を確実に収集し統計/状態を失わずに安全終了する。

手順:
1. 停止要求トリガ (ユーザ操作 / 例外 / シグナル / テスト)
2. 全 control_queue に `ControlMessage(type=STOP)` を非ブロッキング投入 (失敗は警告のみ)
3. 50ms 程度の猶予 sleep で Worker が StatsMessage / ExitNotice を投入できる時間を確保
4. 各 Worker Thread/Process を join。process join timeout 超過: terminate()→短待機→still alive なら kill()。
5. `stop_event.set()` を呼び補助スレッド(dispatcher / metrics / ping) を終了方向へ。
6. dispatcher / metrics / ping thread を順次 join。
7. 収集された ExitNotice を `Orchestrator.exit_notices` で公開 (GUI/テスト用)。
8. (placeholder) enable_central_logging 有効時の集中ログ終了処理は後続実装。
9. 完了時 `SHUTDOWN_COMPLETE` ログ (workers 数含む)。

設計要点:
- dispatcher は step5 まで稼働し続けるため ExitNotice を失わない。
- 旧シーケンスの result_queue SENTINEL は不要となりコード単純化。
- プロセスモードも同じ高レベルシーケンスを採用 (terminate 実装は D-07 オープン事項として継続)。
### 3.10 OrchestratorConfig 追加フィールド (v0.5)
| フィールド | 型 | 目的 |
|-----------|----|------|
| stop_grace_wait_sec | float | STOP ブロードキャスト後 ExitNotice/Stats flush 猶予 |
| simulate_hang_on_stop | bool | テスト用: プロセス側 STOP 後 sleep し join timeout を誘発 |
| enable_central_logging | bool | 集中ロギング有効化プレースホルダ (未実装) |

### 3.11 health_state 追加キー
| キー | 説明 |
|------|------|
| last_rtt_ms | 直近 ping RTT (ms) None=未測定 |

---
## 4. 例外クラス (errors.py)
```python
class ConfigValidationError(Exception):
  """設定値が不正。"""
class StreamConnectionError(Exception):
  """RTSP接続に失敗。再試行可。"""
class ModelLoadError(Exception):
  """モデルロード致命的失敗。"""
class InferenceError(Exception):
  """単フレーム推論失敗 (継続可能)"""
class IPCChannelError(Exception):
  """制御Pipe/Queue の異常 (切断/破損)。"""
```
Fatal 判定: ConfigValidationError, ModelLoadError → 起動中断。

---
## 5. ログポリシー具体化
| コンテキスト | ログレベル | メッセージ例 | event_code |
|--------------|------------|--------------|------------|
| 起動完了 | INFO | Startup complete cameras=3 | STARTUP_COMPLETE |
| 再接続 | WARNING | Reconnecting camera=cam01 attempt=2 | CAPTURE_RETRY |
| 推論失敗(単発) | ERROR | Inference failed camera=cam02 err=... | INFER_FAIL |
| モデルロード失敗 | CRITICAL | Model load failed path=... | MODEL_LOAD_ERROR |
| 性能遅延 | DEBUG | Latency high camera=cam01 ms=180 | PERF_LATENCY |
| キュー飽和 | WARNING | Result queue full camera=cam01 drop=1 | RESULT_DROP |
| ストール | WARNING | Camera stalled camera=cam02 last_update=... | CAMERA_STALL |
| 子終了 | INFO | Worker exit camera=cam01 code=0 | WORKER_EXIT |
| join timeout | WARNING | worker process join timeout | WORKER_JOIN_TIMEOUT |
| 強制終了(kill) | ERROR | worker process still alive after terminate; killing | WORKER_FORCE_KILL |

---
## 6. シーケンス (例: 正常起動→解析→録画→停止)
```
User -> main.py -> ConfigLoader.load
 -> Orchestrator.start
   -> spawn_worker(cam01..N)
   -> each worker: capture loop -> infer -> queue -> Aggregator.push
 -> GUI: poll Aggregator/build_dashboard_state -> render
User: start recording cam01 -> Recorder.start -> file open
User: stop recording cam01 -> Recorder.stop -> write metadata
User: exit -> Orchestrator.stop -> workers stop -> flush -> crash? none -> exit
```

---
## 7. フロー(異常系) 例: カメラ接続断
```
Worker capture fail -> retry (backoff)
  -> attempts exceed -> send status=DOWN to Orchestrator
  -> Orchestrator logs WARNING/ERROR
  -> (将来) GUI indicates reconnect
```

---
## 8. パフォーマンス設計詳細
| 項目 | 方針 | 測定方法 |
|------|------|----------|
| 推論レイテンシ | 前処理+推論時間計測 | time.perf_counter 差分 (内部は monotonic) |
| FPS | Aggregator が一定期間内の件数集計 | 1s ウィンドウ |
| EMA FPS | 指数移動平均 (alpha=0.2) | snapshot_stats 内で更新 |
| レイテンシ分布 | p50/p95 (1s 窓) | 補間方式: 線形 (位置=(n-1)*p) |
| メモリ | 各プロセスモデル1個 | OSツール/psutil (後) |
| ドロップ率 | キャプチャ受信数 vs 推論送信数 | カウンタ差分 |
| ストール検知閾値 | last_result_age > 3/target_fps +1s | metrics thread |

---
## 9. テスト詳細観点 (補足)
| コンポーネント | Unit | Integration | Performance |
|----------------|------|-------------|-------------|
| ConfigLoader | XML→Config 正常/欠落値 | - | - |
| Orchestrator | spawn停止制御 | 複数ワーカー起動 | - |
| Worker | 前処理/ラベル整形/結果投入ドロップ制御 | RTSP疑似ソース | 推論レイテンシ |
| Aggregator | push/query/export/統計融合 | End-to-end カメラ2台 | - |
| Recorder | start/stop メタ生成 | 擬似フレーム供給 | 書込速度(任意) |
| Logging | QueueHandler -> Listener JSON構造 | 子2台並列ログ | - |
| Shutdown | graceful + terminate fallback + RTT 計測 | terminate 経路 | - |

---
## 10. 実装優先順位 (MVPスライス)
1. ConfigLoader + messages + logging_setup (Queue) + 単一カメラ Worker + Aggregator + dispatcher thread (CLI統計)
2. 複数カメラ + StatsMessage 統計表示 + ドロップ制御
3. GUIAdapter + 仮GUI(簡易表示)
4. Recorder 基本機能
5. エクスポート機能
6. 健全性 (ストール検知) & graceful shutdown 強化

---
## 11. Copilot プロンプト用最小入力テンプレ
```
【Goal】基本/詳細設計をコード化 (MVP → 拡張)。
【Focus】ConfigLoader -> messages -> logging_setup -> Orchestrator -> Worker -> Aggregator MVP
【Constraints】要件/NFR遵守, pathlib, env禁止, Google Docstring, 90% tests, 集中ログ, IPC メッセージ型使用。
【Interfaces】load(), start()/stop()/broadcast(), push(), query(), export(), snapshot_stats()
【Error Policy】設定/モデル致命=即停止, 接続断=再試行, 単発推論エラー=継続, IPC断=再起動/停止。
【Deliverables】コード+pytest+使用例+改善提案
```

---
## 12. オープン事項 (詳細設計レベル)
| ID | 内容 | 対応 |
|----|------|------|
| D-01 | RTSP 取得ライブラリ最適案 (opencv vs gstreamer) | PoC 比較 |
| D-02 | GUI イベントループ統合方法 | 実装時検証 |
| D-03 | Recorder パイプライン方式 (raw frame or encoded queue) | 選定 |
| D-04 | メタデータ書式拡張 (モデルハッシュ) | 後続検討 |
| D-05 | メッセージシリアライズ最適化 (pickle→msgpack) | 性能計測後 |
| D-06 | OpenTelemetry Exporter/OTLP | 要件追加時 |

---
## 13. 変更管理
- 本書更新は PR (label: detailed-design)。

---
(以上)
